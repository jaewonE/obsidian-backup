## 개요
![[다중 로지스틱 회귀의 한계]]

위와 같은 한계는 $Pr(Y=k|X=x)$를 직접 모델링 하여 발생하는 문제이다. 이에 베이즈 정리를 사용하여 구한 $Pr(Y=k|X=x)$의 추정치를 통해 모델링을 하는 선형판별분석을 대안적으로 사용한다.

## 정의
**선형판별분석(Linear Discriminant Analysis : LDA)** 은 데이터 분포를 학습해 **결정경계(Decision boundary)** 를 만들어 데이터를 **분류(classification)** 하는 모델이다. 이에 선형판별분석은 아래와 같은 순서로 진행된다. 

1. 반응변수 Y의 각 클래스에서 설명변수 X의 분포를 모델링하고
2. 베이즈 정리를 사용하여 $Pr(Y=k|X=x)$ 에 추정치를 얻는다.
3. 이 분포들이 정규분포라고 가정 할 경우 모델은 로지스틱 회귀와 형태가 아주 비슷하다. 

<br>
### 베이즈 정리를 이용한 분류
베이즈 정리를 이용한 모델인 [[베이즈 분류기]]는 사후확률(관측치 $X=x$가 k번째 클래스에 속할 확률) 계산시, 가장 낮은 오차율을 가진다. 그러므로 설명변수 X를 베이즈 정리를 이용한 함수  $Pr(Y=k|X=x)$ 에 적합하여 낮은 오차율을 가지는 베이즈 분류기 모델을 생성할 수 있다.

베이즈 정리를 이용한 분류 함수를 정의하기 전 용어 정의를 먼저 알아보자.
* $\pi_k$ : 사전확률(prior probability) 또는 전체 확률 : 무작위로 선택된 관측치 k번째 클래스에서 나올 확률로 주어진 관측치가 반응변수 Y의 k번째 범주와 연관되어 있을 확률이다. 
* $f_k(X)$ : k 번째 클래스에 속하는 관측치에 대한 X의 밀도함수(density function)으로 $Pr(X=x|Y=k)$와 같다.
* $p_k(x)$ : 사후확률(posterior probablitiy) : 관측치 $X=x$가 k번째 클래스에 속하는 확률로 관측치에 대한 주어진 설명변수값에 대해 그 관측치가 k번째 클래스에 속하는 확률.

위 용어 정의들을 통해 구성된 설명변수 X에 대한 베이즈 정리는 아래와 같다.
$$p_k(x) = Pr(Y=k|X=x) = \cfrac{\pi_kf_k(x)}{\sum^K_{l=1}\pi_lf_l(x)}$$
X에 대한 베이즈 정리를 통해 다음 사실을 알 수 있다.
* k번째 클래스의 관측치 값이 X에 거이 근접할 때($X \approx x$ 의 확률이 높을 때) $f_k(x)$의 값이 상대적으로 크다. 
* k번째 클래스의 관측치 값이 X에 근접 하지 않을 때($X \approx x$ 일 가능성이 낮을 때) $f_k(x)$의 값이 상대적으로 작다. 

위와 같은 X에 대한 베이즈 정리를 이용하여 설명변수가 1개일 때와 2개 이상일 때를 나누어 베이즈 분류기를 적용하여 보자.
<br>
## 설명변수가 1개인 경우
설명변수가 1개인 경우에 판별함수와 선형판변분석 분류기에 대하여 알아보고 그래프를 보고 해석해보자.

### 판별함수
설명변수가 1개 일 때 사후확률(  $p_k(x)$ )이 최대가 되는 클래스로 관측치를 분류하기 위해서 밀도함수(  $f_k(X)$ )를 1차원의 정규밀도함수(정규분포 또는 가우스 분포)로 가정하자. 
밀도함수(  $f_k(X)$ ) 는 아래와 같고
$$f_k(x)=\frac{1}{\sqrt{2 \pi} \sigma_k} \exp \left(-\frac{1}{2 \sigma_k^2}\left(x-\mu_k\right)^2\right)$$
> * $\mu_k$ 와 $\sigma_k^2$ 는 k번째 클래스에 대한 평균과 분산이다.
> * 위 식은 모든 K개 클래스에 대한 공통을 분산($\sigma^2_1 = \sigma^2_2 = ... = \sigma^2_K$)이 있다고 가정하며 그 값을 $\sigma^2$ 이라 한다.

이를 베이즈 정리에 대입하면 아래와 같다.
$$p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^2}\left(x-\mu_k\right)^2\right)}{\sum_{l=1}^K \pi_l \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^2}\left(x-\mu_l\right)^2\right)}$$

베이즈 분류기는 베이즈 정리 값인 사후확률이 최대가 되는 클래스 관측치 $X=x$를 할당하는 것이다. 따라서 위 식을 정리한 아래식의 값을 최대로 하는 클래스에 관측치를 할당하는 것과 동일하다. 
$$\delta_k(x)=x \cdot \frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2 \sigma^2}+\log \left(\pi_k\right)$$
위 식을 **판별함수** 라고 부르고  $\delta_k(x)$ 라고 표기한다.
> 앞으로 판별함수 추정치와 판별함수 모두 구분하지 않고 판별함수라고 부르겠다.

판별함수(  $\hat{\delta}_k(x)$  )을 구하기 위해서는 $\mu_k\,,\,\sigma^2\,,\,\pi_k$ 로 총 3개의 변수에 대한 추정값이 필요하다. 이때 사용하는 것이 선형판별분석 분류기이다.
<br>
### 선형판별분석 분류기
선형판별분석분류기(LDA 분류기)는 $\mu_k\,,\,\sigma^2\,,\,\pi_k$ 에 대한 추정값을 판별함수에 대입하여 베이즈 분류기를 근사하는 것이다. 이 때 다음 추정치가 사용된다.
$$\begin{aligned} \hat{\mu}_k & =\frac{1}{n_k} \sum_{i: y_i=k} x_i \\ \hat{\sigma}^2 & =\frac{1}{n-K} \sum_{k=1}^K \sum_{i: y_i=k}\left(x_i-\hat{\mu}_k\right)^2 \\ \hat{\pi}_k & =\cfrac{n_k}{n} \end{aligned}$$
>* n은 총 훈련 관측치의 개수
>* $n_k$는 k번째 클래스의 훈련 관측치 수
>* $\hat{\mu}_k$는 k번째 클래스 내 모든 훈련 관측치들의 평균
>* $\hat{\sigma^2}$는 K개 클래스 각각에 대한 표본분산의 가중평균

즉 LDA 분류기는 아래 판별함수가 최대가 되는 클래스에 관측치 $X=x$를 할당한다. 
$$\hat{\delta}_k(x)=x \cdot \frac{\hat{\mu}_k}{\hat{\sigma}^2}-\frac{\hat{\mu}_k^2}{2 \hat{\sigma}^2}+\log \left(\hat{\pi}_k\right)$$
>위 판별함수(  $\hat{\delta}_k(x)$ )가 x의 선형함수인 탓에 분류기 이름에 **선형** 이라는 말이 붙었다.

정리하면, LDA 분류기는 각 클래스 내의 관측치들이 클래스 특정(클래스 별) 평균벡터와 클래스 공통의 분산($\sigma^2$)을 갖는 정규분포를 따른다는 가정하에 이 파라미터들에 대한 추정값을 베이즈 분류기에 대입하여 얻는다. 

### 그래프 확인
그래프를 통해 위 내용을 확인해보자. 
아래 그래프에서 왼쪽은 2차원 정규밀도함수이고 수직 파선은 [[베이즈 분류기|베이즈 결정경계]]이다. 
오른쪽은 두 클래스에서 20개의 관측치를 각각 추출하여 히스토그램으로 나타낸 것으로 수직 파선은 베이즈 결정경계이고 수직 실선은 LDA 결정경계이다.
![[4.4.jpg]]
* 왼쪽 정규밀도함수에서 관측치가 두 클래스에 속할 가능성이 동일할 때( $\pi_1 = \pi_2 = 0.5$ ) 베이즈 분류기는 $x<0$ 인 경우 관측치를 클래스 1(초록색)에 할당하고 그렇지 않으면 클래스 2(붉은색)에 할당한다. 
* LDA 결정경계는 베이즈 결정경계보다 약간 왼쪽에 있는 것을 통해 LDA가 이 데이터셋에 대해 아주 잘 동작한다는 것을 나타낸다. 실제로 베이즈 오차율과 LDA 검정오차율을 계산하였을 때 각각 10.6%와 11.1%로 유사하게 나온 것을 확인 할 수 있다.
<br>
## 설명변수가 2개 이상인 경우
