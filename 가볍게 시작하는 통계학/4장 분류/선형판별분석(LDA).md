## 개요
![[다중 로지스틱 회귀의 한계]]

위와 같은 한계는 $Pr(Y=k|X=x)$를 직접 모델링 하여 발생하는 문제이다. 이에 베이즈 정리를 사용하여 구한 $Pr(Y=k|X=x)$의 추정치를 통해 모델링을 하는 선형판별분석을 대안적으로 사용한다.

## 정의
**선형판별분석(Linear Discriminant Analysis : LDA)** 은 데이터 분포를 학습해 **결정경계(Decision boundary)** 를 만들어 데이터를 **분류(classification)** 하는 모델이다. 이에 선형판별분석은 아래와 같은 순서로 진행된다. 

1. 반응변수 Y의 각 클래스에서 설명변수 X의 분포를 모델링하고
2. 베이즈 정리를 사용하여 $Pr(Y=k|X=x)$ 에 추정치를 얻는다.
3. 이 분포들이 정규분포라고 가정 할 경우 모델은 로지스틱 회귀와 형태가 아주 비슷하다. 



### 베이즈 정리를 이용한 분류
베이즈 정리를 이용한 모델인 [[베이즈 분류기]]는 사후확률(관측치 $X=x$가 k번째 클래스에 속할 확률) 계산시, 가장 낮은 오차율을 가진다. 그러므로 설명변수 X를 베이즈 정리를 이용한 함수  $Pr(Y=k|X=x)$ 에 적합하여 낮은 오차율을 가지는 베이즈 분류기 모델을 생성할 수 있다.

베이즈 정리를 이용한 분류 함수를 정의하기 전 용어 정의를 먼저 알아보자.
* $\pi_k$ : 사전확률(prior probability) 또는 전체 확률 : 무작위로 선택된 관측치 k번째 클래스에서 나올 확률로 주어진 관측치가 반응변수 Y의 k번째 범주와 연관되어 있을 확률이다. 
* $f_k(X)$ : k 번째 클래스에 속하는 관측치에 대한 X의 밀도함수(density function)으로 $Pr(X=x|Y=k)$와 같다.
* $p_k(x)$ : 사후확률(posterior probablitiy) : 관측치 $X=x$가 k번째 클래스에 속하는 확률로 관측치에 대한 주어진 설명변수값에 대해 그 관측치가 k번째 클래스에 속하는 확률.

위 용어 정의들을 통해 구성된 설명변수 X에 대한 베이즈 정리는 아래와 같다.
$$p_k(x) = Pr(Y=k|X=x) = \cfrac{\pi_kf_k(x)}{\sum^K_{l=1}\pi_lf_l(x)}$$
X에 대한 베이즈 정리를 통해 다음 사실을 알 수 있다.
* k번째 클래스의 관측치 값이 X에 거이 근접할 때($X \approx x$ 의 확률이 높을 때) $f_k(x)$의 값이 상대적으로 크다. 
* k번째 클래스의 관측치 값이 X에 근접 하지 않을 때($X \approx x$ 일 가능성이 낮을 때) $f_k(x)$의 값이 상대적으로 작다. 

위와 같은 X에 대한 베이즈 정리를 이용하여 설명변수가 1개일 때와 2개 이상일 때를 나누어 베이즈 분류기를 적용하여 보자.
<br>
## 설명변수가 1개인 경우
설명변수가 1개인 경우에 판별함수와 선형판변분석 분류기에 대하여 알아보고 그래프를 보고 해석해보자.

### 판별함수
설명변수가 1개 일 때 사후확률(  $p_k(x)$ )이 최대가 되는 클래스로 관측치를 분류하기 위해서 밀도함수(  $f_k(X)$ )를 1차원의 정규밀도함수(정규분포 또는 가우스 분포)로 가정하자. 
밀도함수(  $f_k(X)$ ) 는 아래와 같고
$$f_k(x)=\frac{1}{\sqrt{2 \pi} \sigma_k} \exp \left(-\frac{1}{2 \sigma_k^2}\left(x-\mu_k\right)^2\right)$$

> * $\mu_k$ 와 $\sigma_k^2$ 는 k번째 클래스에 대한 평균과 분산이다.
> * 위 식은 모든 K개 클래스에 대한 공통을 분산($\sigma^2_1 = \sigma^2_2 = ... = \sigma^2_K$)이 있다고 가정하며 그 값을 $\sigma^2$ 이라 한다.

이를 베이즈 정리에 대입하면 아래와 같다.
$$p_k(x)=\frac{\pi_k \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^2}\left(x-\mu_k\right)^2\right)}{\sum_{l=1}^K \pi_l \frac{1}{\sqrt{2 \pi} \sigma} \exp \left(-\frac{1}{2 \sigma^2}\left(x-\mu_l\right)^2\right)}$$

베이즈 분류기는 베이즈 정리 값인 사후확률이 최대가 되는 클래스 관측치 $X=x$를 할당하는 것이다. 따라서 위 식을 정리한 아래식의 값을 최대로 하는 클래스에 관측치를 할당하는 것과 동일하다. 
$$\delta_k(x)=x \cdot \frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2 \sigma^2}+\log \left(\pi_k\right)$$
위 식을 **판별함수** 라고 부르고  $\delta_k(x)$ 라고 표기한다.
> 앞으로 판별함수 추정치와 판별함수 모두 구분하지 않고 판별함수라고 부르겠다.

판별함수(  $\hat{\delta}_k(x)$  )을 구하기 위해서는 $\mu_k\,,\,\sigma^2\,,\,\pi_k$ 로 총 3개의 변수에 대한 추정값이 필요하다. 이때 사용하는 것이 선형판별분석 분류기이다.
<br>
### 선형판별분석 분류기
선형판별분석분류기(LDA 분류기)는 $\mu_k\,,\,\sigma^2\,,\,\pi_k$ 에 대한 추정값을 판별함수에 대입하여 베이즈 분류기를 근사하는 것이다. 이 때 다음 추정치가 사용된다.
$$\begin{aligned} \hat{\mu}_k & =\frac{1}{n_k} \sum_{i: y_i=k} x_i \\ \hat{\sigma}^2 & =\frac{1}{n-K} \sum_{k=1}^K \sum_{i: y_i=k}\left(x_i-\hat{\mu}_k\right)^2 \\ \hat{\pi}_k & =\cfrac{n_k}{n} \end{aligned}$$
>* n은 총 훈련 관측치의 개수
>* $n_k$는 k번째 클래스의 훈련 관측치 수
>* $\hat{\mu}_k$는 k번째 클래스 내 모든 훈련 관측치들의 평균
>* $\hat{\sigma^2}$는 K개 클래스 각각에 대한 표본분산의 가중평균

즉 LDA 분류기는 아래 판별함수가 최대가 되는 클래스에 관측치 $X=x$를 할당한다. 
$$\hat{\delta}_k(x)=x \cdot \frac{\hat{\mu}_k}{\hat{\sigma}^2}-\frac{\hat{\mu}_k^2}{2 \hat{\sigma}^2}+\log \left(\hat{\pi}_k\right)$$
>위 판별함수(  $\hat{\delta}_k(x)$ )가 x의 선형함수인 탓에 분류기 이름에 **선형** 이라는 말이 붙었다.

정리하면, LDA 분류기는 각 클래스 내의 관측치들이 클래스 특정(클래스 별) 평균벡터와 클래스 공통의 분산($\sigma^2$)을 갖는 정규분포를 따른다는 가정하에 이 파라미터들에 대한 추정값을 베이즈 분류기에 대입하여 얻는다. 

### 그래프 확인
그래프를 통해 위 내용을 확인해보자. 
아래 그래프에서 왼쪽은 2차원 정규밀도함수이고 수직 파선은 [[베이즈 분류기|베이즈 결정경계]]이다. 
오른쪽은 두 클래스에서 20개의 관측치를 각각 추출하여 히스토그램으로 나타낸 것으로 수직 파선은 베이즈 결정경계이고 수직 실선은 LDA 결정경계이다.
![[4.4.jpg]]
* 왼쪽 정규밀도함수에서 관측치가 두 클래스에 속할 가능성이 동일할 때( $\pi_1 = \pi_2 = 0.5$ ) 베이즈 분류기는 $x<0$ 인 경우 관측치를 클래스 1(초록색)에 할당하고 그렇지 않으면 클래스 2(붉은색)에 할당한다. 
* LDA 결정경계는 베이즈 결정경계보다 약간 왼쪽에 있는 것을 통해 LDA가 이 데이터셋에 대해 아주 잘 동작한다는 것을 나타낸다. 실제로 베이즈 오차율과 LDA 검정오차율을 계산하였을 때 각각 10.6%와 11.1%로 유사하게 나온 것을 확인 할 수 있다.
<br>
## 설명변수가 2개 이상인 경우
기본적인 형태는 설명변수가 1개 일 때와 유사하다. 설명변수가 2개 이상 경우에 판별함수와 선형판변분석 분류기에 대하여 알아보고 그래프를 보고 해석해보자.

### 판별함수
설명변수가 1개 일 때 사후확률(  $p_k(x)$ )이 최대가 되는 클래스로 관측치를 분류하기 위해서 밀도함수를 가우스 분포로 가정하였었다. 설명변수가 2개 이상일 때는 밀도함수(  $f_k(X)$ )를 [[다변량 가우스분포]]로 가정한다.

>이때 다변량 가우스분포는 $N(\mu_k,\boldsymbol{\Sigma})$ 을 따르는데 $\mu_k$는 클래스 특정 평균벡터이고 $\boldsymbol{\Sigma}$는 클래스 모두에게 공통인 공분산 행렬이다.

다변향 가우스분포를 밀도함수로 하는 베이즈 분류기는 아래 식의 값이 최대가 되었을 때 사후확률(베이즈 정리 값)이 최대가 되는 클래스 관측치 $X=x$를 할당한다.
$$\delta_k(x)=x^T \boldsymbol{\Sigma}^{-1} \mu_k-\frac{1}{2} \mu_k^T \boldsymbol{\Sigma}^{-1} \mu_k+\log \pi_k$$
설명변수가 1개 일 때와 동일하게 **판별함수** 라고 부르고  $\delta_k(x)$ 라고 표기한다.
> 동일하게 판별함수 추정치와 판별함수 모두 구분하지 않고 판별함수라고 부르겠다.

판별함수(  $\hat{\delta}_k(x)$  )을 구하기 위해서는 $\mu_k\,,\,\boldsymbol{\Sigma}\,,\,\pi_k$ 로 총 3개의 변수에 대한 추정값이 필요하다. 이때 사용하는 것이 선형판별분석 분류기이다.

### 선형판별분석 분류기
선형판별분석분류기(LDA 분류기)는$\mu_k\,,\,\boldsymbol{\Sigma}\,,\,\pi_k$ 에 대한 추정값을 판별함수에 대입하여 베이즈 분류기를 근사하는 것이다. 이 때 다음 추정치가 사용된다.
$$\begin{aligned} \hat{\mu}_k & =\frac{1}{n_k} \sum_{i: y_i=k} x_i \\ \hat{\pi}_k & =\cfrac{n_k}{n} \end{aligned}$$
>* n은 총 훈련 관측치의 개수
>* $n_k$는 k번째 클래스의 훈련 관측치 수
>* $\hat{\mu}_k$는 k번째 클래스 내 모든 훈련 관측치들의 평균

#질문 공분산행렬($\boldsymbol{\Sigma}$)을 계산하는 방법에 대하여 교재에 나와있지 않다.

즉 LDA 분류기는 아래 판별함수가 최대가 되는 클래스에 관측치 $X=x$ 를 할당한다. 
$$\hat{\delta}_k(x)=x^T \hat{\boldsymbol{\Sigma}}^{-1} \hat{\mu}_k-\frac{1}{2} \hat{\mu}_k^T \hat{\boldsymbol{\Sigma}}^{-1} \hat{\mu}_k+\log \hat{\pi}_k$$
>위 판별함수(  $\hat{\delta}_k(x)$ )가 x의 선형함수인 탓에 분류기 이름에 **선형** 이라는 말이 붙었다.

### 그래프 확인

그래프로 확인해보자. 아래는 세 개의 클래스를 가진 예로 각 클래스의 관측치들은 클래스의 특정 평균벡터와 공통의 공분산 행렬을 갖고 두 개의 설명변수(p=2)를 가지는 다변량가우스분포로 부터 선택된다.
왼쪽은 세 클래스 각각에 대한 95% 확률을 포함하는 타원이고 파선은 베이즈 결정경계이다.
오른쪽은 각 클래스에서 선택된 20개 관측치와 실선의 LDA 결정경계, 파선의 베이즈 결정경계이다. 
![[4.6.jpg]]

* 베이즈 결정경계는 서로 다른 클래스의 두 판별함수 $\hat{\delta}_k(x)\,,\,\hat{\delta}_l(x)$ 가 같아지는 지점으로 아래 식이 만족하는 x 값의 집합을 나타낸다.(로그항은 상쇄되어 제거되었다.)
	$$x^T \hat{\boldsymbol{\Sigma}}^{-1} \hat{\mu}_k-\frac{1}{2} \hat{\mu}_k^T \hat{\boldsymbol{\Sigma}}^{-1} \hat{\mu}_k=x^T \hat{\boldsymbol{\Sigma}}^{-1} \hat{\mu}_l-\frac{1}{2} \hat{\mu}_l^T \hat{\boldsymbol{\Sigma}}^{-1} \hat{\mu}_l$$
* 오른쪽 도표에서 전반적으로 LDA 결정경계는 베이즈 결정경계에 상당히 가깝다. 이는 LDA가 이 데이터에 대해 잘 동작한다는 것을 나타낸다. 

### 임계치 변경에 따른 LDA 변화

선형판별분석 모델(LDA)는 판별함수를 통해 베이즈 분류기를 근사한다. 이에 선형판별분석 모델은 베이즈 분류기의 총오차율이 최소화된다는 특징을 가지고 있다. 하지만 이러한 특징에 **LDA는 각 클래스의 오차율을 고려하지 않는다는 한계**를 수반한다. 

아래 자료를 통해 LDA의 한계를 알아보자. 아래 자료는 10000명의 사람들에 대하여 실제 연체 상태와 모델이 예측한 상태를 표로 나타낸 혼동행렬(confusion matrix) 자료이다. 

|       | No   | Yes | Total      |
| ----- | ---- | --- | ----- |
| No    | 9644 | 252 | 9896  |
| Yes   | 23   | 81  | 104   |
| Total | 9667 | 333 | 10000 |

<행은 실제 연체 상태이고 열을 예측한 연체 상태이다 (기본 임계치는 0.5 즉, $Pr(연체=Yes | X=x) > 0.5$ 으로 측정한 값이다)>

* 연체를 하고 했다고 예측 : 81
* 연체를 하고 하지 않았다고 예측 : 252
* 연체를 하지 않았고 했다고 예측 : 23
* 연체를 하지 않았고 하지 않았다고 예측: 9644

전체 오류율과 각 오류율은 아래와 같다.
* 전체 오류율 : 2.75% = ((23 + 252) / 10000) * 100 (%)
* 연체한 사람들의 오류율 : 75.7% = (252 / 333) * 100 (%)
* 연체를 하지 않은 사람들의 오류율 : 0.23% = (23 / 9667) * 100 (%)

위 자료에서 연체를 한 사람(333명) 중 252명을 잘못 불류하였다. 전체 오차율은 낮지만 연체한 사람들의 오류율은 $252 / 333 \approx 75.7$%로 상당히 높음을 알 수 있다. 이는 **LDA는 전체 오차율을 낮추는 것에 초점을 두지만 개별 클래스의 오차율에는 관심이 없기 때문**이다.

위와 같은 현상은 **관측치를 클래스로 할당하는 임계치를 조절**함으로써 해결할 수 있다. 
예를 들어 위 예시에 대해서 연체의 사후확률이 20%를 초과하는 사람은 모두 연체 클래스로 분류하도록 아래와 같이 임계치를 설정하였을 때
$$P(연체 = Yes | X=x) > 0.2$$
아래와 같은 결과를 얻었다. 

|       | No   | Yes | Total |
| ----- | ---- | --- | ----- |
| No    | 9432 | 138 | 9570  |
| Yes   | 235  | 195 | 430   | 
| Total | 9667 | 333 | 10000 |
<행은 실제 연체 상태이고 열을 예측한 연체 상태이다 (임계치는 0.2 즉, $Pr(연체=Yes | X=x) > 0.2$ 으로 측정한 값이다)>

* 연체를 하고 했다고 예측 : 195
* 연체를 하고 하지 않았다고 예측 : 138
* 연체를 하지 않았고 했다고 예측 : 235
* 연체를 하지 않았고 하지 않았다고 예측: 9432

전체 오류율과 각 오류율은 아래와 같다.
* 전체 오류율 : 3.73% = ((235 + 138) / 10000) * 100 (%)
* 연체한 사람들의 오류율 : 41.4% = (138 / 333) * 100 (%)
* 연체를 하지 않은 사람들의 오류율 : 2.43% = (235 / 9667) * 100 (%)
* 
위 결과에 대하여 아래와 같은 사실을 알 수 있다.
* 임계치 값을 변경하였기에 예측한 인원수는 변경되었지만 실제 연체 인원수는 변경되지 않았다.
* 연체한 사람들의 오류율은 $138 / 333 \approx 41.4$%로 기본 임계치(0.5)를 설정했을 때(75.7%) 보다 현저히 작아졌다.
* 전체 오차율은 3.73%로 기존 2.75%에서 소폭 상승하였다.

즉 트레이드 오프(trade-off)인 것이다. 각 클래스의 오차율을 줄일려고 하면 전체 오차율이 증가하고 전체 오차율을 감소시키면 각 클래스에 대한 오차율을 조절할 수 없다. 이와 같은 임계치에 따른 트레이드 오프를 보여주는 그래프가 바로 ROC 그래프이다.
![[4.8.jpg|500]]

위와 같은 모양을 가지는 ROC(Receiver operating character-istics)으로 한국어로 **수신기 동작 특성** 이라고 부른다. 
ROC 그래프는 동일한 임계치를 사용하여 민감도와 특이도의 비율을 나타낸 것이다. (y=x 직선은 임계치가 0.5일 때(정보가 없는 분류기) ROC 그래프이다.)

> 민감도: 실제로 참이면서 참으로 예측한 비율(예: 위의 첫번째 혼돈행렬에서 민감도는 24.3% 이다.)
> 특이도: 실제로 거짓이면서 거짓으로 예측한 비율(예: 위의 첫번째 혼돈행렬에서 특이도는 99.8% 이다.)

ROC 그래프의 곡선 아래의 면적은 분류기의 전체 성능으로 AUC 라고 부르고 AUC가 클수록 더 좋은 분류기이다. 즉, 위와 같이 곡선이 좌측 상단에 가깝게 위치할수록 이상적인 그래프인 것이다. (만약 임계치가 0.5(직선 그래프) 일 경우에는 AUC가 0.5 일 것이다.)

위 모델의 ROC 그래프를 그려 모델의 오차율을 예상하고 적절한 임계치를 설정 할 수 있다.

![[이차선형판별분석(QDA)]]