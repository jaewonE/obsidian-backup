### KNN(K-Nearest Neighbors), K-최근접이웃

#### 개요
이론상 질적 반응변수는 베이즈 분류기를 사용하여 예측하는 것이 항상 가장 좋다. 그러나, 실제 데이터에서는 주어진 **X에 대한 Y의 조건부 분포를 모르므로** 베이즈 분류기를 계산 할 수 없다. 이에 베이즈 분류기는 다른 방법들을 비교하는 데 사용되는 달성할 수 없는 표준 역할을 한다. 

많은 기법들이 주어진 X에 대한 Y의 조건부분포를 추정하여 가장 높은 추정확률을 가지는 클래스로 관측치를 분류하고자 한다. 이러한 방법 중 하나가 KNN이다. 

#KNN
#### KNN
양의 정수 K와 검정 관측치 $x_0$에 대해 KNN분류기는 먼저 훈련 데이터에서 $x_0$에 가장 가까운 K개 점$(=N_0)$을 식별한다. 그 다음에, 클래서 j에 대한 조건부확률을 반응변수 값이 j인 $N_0$ 내 점들의 비율로 추정한다. 수식은 아래와 같다.$$Pr(Y=j|X=x_0)=\frac1K\sum_{i\in N_0}I(y_i=j)$$
그래프로 보자. 좌측 그래프는 KNN을 3으로 설정하였을 때임의의 점(x표시)의 K-이웃(초록색 원)을 나타낸 것이며 우측 그래프는 KNN 결정경계를 기준으로 영역을 나타낸 것이다.
![[FIGURE 2.14.png|1000]]
K-이웃(초록색 원) 내부에 파랑색 값이 2개, 주황색 값이 1개 있음으로 임의의 점(x표시)의 파랑색 클래스의 추정확률이 2/3이고 주황색 클래스의 추정확률이 1/3이라고 해석할 수 있고 이를 토대로 우측과 같이 나타낼 수 있다. 

위와 같은 K값을 적절하게 선택하였을 때, 베이즈 분류기에 놀라울 만큼 가까운 결과(분류)를 도출한다. 
<각각 100개의 값의 클래스 분류에 있어 보라색 점선은 베이즈 결정경계이며 검정색 선은 KNN 결정경계이다 >
![[FIGURE 2.15.png|800]]
K가 10일 때 KNN 결정경계는 베이즈 결정경계와 유사한 것을 확인 할 수 있다.

만약 K값이 지나치게 작을 경우 KNN의 결정경계는 지나치게 유연하여 편향은 낮지만 분산이 높은 분류기가 된다. 
K값이 증가함에 따라 유연성은 작아지며 선형에 가까운 결정경계를 제공한다.

아래는 K가 1일때와 100일때의 KNN 결정경계와 베이즈 결정경계이다. K값에 따른 결정경계를 확인 할 수 있다.
![[FIGURE 2.16.png|1000]]

회귀문제와 같이 KNN 또한 훈련오차율과 검정오차율 사이에 강한 상관관계는 없다. K=1일 때, 훈련오차율은 0이지만, 검정오차율은 상당히 높을 것이다. 일반적으로, 유연성이 높은 분류방법을 사용할 수록 훈련오차율은 감소할 것이지만 검정오차율은 그렇지 않을 수도 있다.

### 연관문서
[[베이즈 분류기]]