>설명변수의 형태에 따른 알맞은 통계학습방법을 선택하는 것은 중요하다

- [[#2.2.1 적합의 품질 측정|2.2.1 적합의 품질 측정]]
- [[#2.2.2 편향-분산 절충|2.2.2 편향-분산 절충]]
- [[#2.2.3 분류 설정|2.2.3 분류 설정]]

#MSE
#소불가능오차
## 2.2.1 적합의 품질 측정
통계학습방법의 성능을 평가하기 위해 예측된 반응 값과 실제 값 간의 차이를 수량화하는 것은 중요하다. 이때 많이 사용되는 것이 [[평균 제곱 오차 MSE]] 이다.

아래는 데이터(좌측 검정색 원) 분포에 대한 추정($\hat{f}$) 함수의 적합도(좌측)과 MSE(우측)에 대한 도표이다.(좌측 검정색 선이 실제 $f$이다.) (우측 붉은 선은 기대 검정 MSE이고 회색은 훈련 MSE라고 이다.)
좌측: 데이터(검정색 원) 분포에 대한 실제 함수$f$(검정색)과 추정함수(이외의 색)이다.
우측: 붉은색과 회색 선은 각각 기대 검정 MSE와 훈련 MSE 의 유연성에 따른 분포이다. 
![[FIGURE 2.9.png|1000]]
이를 통해 아래와 같은 사실을 알 수 있다.
* 주황색 추정 함수는 선형 회귀 적합이며 비교적 유연하지 않으며 MSE 또한 높음으로 적합한 추정이 아니다.
* 초록색 추정 함수는 유연하지만 유연성이 증가함에 따라 MSE가 증가함으로 (기대 검정과 훈련 MSE의 차이가 크다) 과적합된 추정으로 적합하지 않다.
	* 기대 검정 MSE가 크면 과적합을 일으킨다.
* 하늘색 추정 함수는 유연하면서 MSE 또한 적음으로 적합한 추정이다.
* **통상적으로 통계학습방법의 유연성이 증가함에 따라 훈련 MSE는 단조감소하는 형상을, 검정 MSE는 U 모양을 보인다.**
	* 전체 기대 검정 MSE의 식은 아래와 같음으로 축소불가능오차(c)보다 작을 수 없다.(분산은 0 이상임으로)(기대 검정 MSE가 U 모양을 보이는 이유)
	  $E\left(y_0-\hat{f}\left(x_0\right)\right)^2=\operatorname{Var}\left(\hat{f}\left(x_0\right)\right)+\left[\operatorname{Bias}\left(\hat{f}\left(x_0\right)\right)\right]^2+\operatorname{Var}(\epsilon)$
	  


#편향-분산_절충
## 2.2.2 편향-분산 절충
* 분산이란 추정 함수($f$)가 변동되는 정도를 의미한다.
* 편향이란 실제 문제를 훨신 단순한 모델로 근사시킴으로서 발생하는 오차이다.

* 원칙적으로 통계학습방법의 유연성이 높을수록 분산은 증가하고 편향이 감소한다.
* 유연성을 증가시킬 때 처음에는 분산의 증가보다 더 빠르게 편향이 감소하는 경향이 있다. 하지만 특정 지점에서 유연성 증가는 편향에 거의 영향이 없지만 분산은 크게 증가시키기 시작한다. 이 지점부터 검정 MSE가 증가한다.
* 
<위 내용은 아래 그래프를 통해 확인 할 수 있다>
 ![[FIGURE 2.12.png|1000]]
 (수직 점선은 검정 MSE가 가장 작아지는 유연성 수준이다)

**위와 같은 편향, 분산, 검정 MSE 사이의 관계를 편향-분산 정충 이라고 한다.**
>통계학습방법이 검정자료에 대해 좋은 성능을 내려면 분산뿐만 아니라 제곱편향도 낮아야 한다. 따라서 **분산과 제곱편향의 절충을 염두해야 한다.**


#훈련오차율
#검정오차율
#베이즈_분류기
#KNN
## 2.2.3 분류 설정
분류 설정에서 가장 흔히 사용되는 모델 정확도 평가 기법은 훈련오차율과 검정오차율이다.
![[훈련오차율#훈련오차율이란]]

![[검정오차율#검정오차율이란]]

이때 검정오차율을 최소로 하는 방법으로 [[베이즈 분류기]] 이론이 제안된다. 하지만 베이즈 분류기는 주어진 X에 대한 Y의 조건부분포를 모르기에 이론상으로만 가능하다.

이에 베이즈 분류기에 근접한 KNN K-최근접이웃 기법이 제안된다.

![[KNN K-최근접이웃#KNN(K-Nearest Neighbors), K-최근접이웃]]