---
title: 'Chapter 7 Lab: Moving Beyond Linearity'
output: rmarkdown::github_document
---


```{r setup, include=FALSE}
rm(list = ls(all = TRUE))

libs <- c("tidyverse", "ISLR", "modelr", "broom", 'splines', 'gam', 'akima')
invisible(lapply(libs, library, character.only = TRUE))
```

Throughout this lab we will be using the same `Wage` data that has been
considered in previous chapters. At a glance, this lab will cover the following
topics:

* Polynomial Regression and Step Functions

* Regression Splines (Natural, Smoothing)

* General Additive Models (GAMs)

## 7.8.1: Polynomial Regression and Step Functions

Polynomial regression is super easy to implement in `R`. We follow the basic 
steps required to do linear regression with `lm()` and adjust some small things
to include polynomial expressions in the formula

Before we begin, just a quick aside - normally I don't use `attach()` in my own 
work because it can get confusing what data is currently attached and which 
columns you are calling. In this scenario, because we are only working with one 
data set through the majority of this lab attaching the data is much less problematic

```{r poly(), warning = F}
# Bring in the Wage data and run a fourth-degree polynomial regression
attach(Wage)
fit <- lm(wage ~ poly(age, 4), data = Wage)
coef(summary(fit))
```

By default, `poly()` returns orthogonal polynomials from degree 1 to degree 4. 
That might seem a little confusing, so we can use the `raw = TRUE` option to specify
that we want _age_, _age^2_, _age^3_, and _age^4_ directly.

```{r poly(raw = TRUE)}
fit2 <- lm(wage ~ poly(age, 4, raw = TRUE), data = Wage)
coef(summary(fit))
```

There are several other ways to do the same thing - you can either use the wrapper
`I()` (as `^` is interpreted as a call to interact variables), or simply just use 
`cbind()`. We've excluded the output from these extra methods, but have included
the code so you can see how they are implemented

```{r alternative polynomial methods, eval = F}
# Using I()
lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data = Wage) %>%
  coef()

# Using cbind()
lm(wage ~ cbind(age, age^2, age^3, age^4), data = Wage) %>%
  coef()
```

Let's take a look at the fitted model to see how well it performs. We'll be using
the `modelr` package to help us add a confidence interval around the fit. Since 
there isn't any difference in outcome between the `poly(..., raw = TRUE)` and previous
two mdoels we will just stick with `fit2` for the time being

```{r model fit, echo = FALSE}
Wage %>%
  augment(fit2, .) %>%
  mutate(fitted_low = .fitted - 2 * .se.fit,
         fitted_high = .fitted + 2 * .se.fit) %>%
  ggplot() +
  geom_point(aes(age, wage), cex = .5) + 
  geom_line(aes(age, .fitted), col = 'blue') +
  geom_line(aes(age, fitted_low), col = 'blue', linetype = 'dashed') +
  geom_line(aes(age, fitted_high), col = 'blue', linetype = 'dashed') +
  labs(title = "Wage fit with degree-4 polynomial") + 
  theme(plot.title = element_text(hjust = .5))
```

We kind of arbitrarily set up this 4-degree polynomial fit, so how do we know 
if its the right one to use? We can use hypothesis testing to determine
which degree polynomial is most appropriate

```{r}
fit1 <- lm(wage~age, data = Wage)
fit2 <- lm(wage~poly(age, 2), data = Wage)
fit3 <- lm(wage~poly(age, 3), data = Wage)
fit4 <- lm(wage~poly(age, 4), data = Wage)
fit5 <- lm(wage~poly(age, 5), data = Wage)

anova(fit1, fit2, fit3, fit4, fit5)
```

Its clear that a linear fit isn't sufficient, and that even a cubic fit would
outperform a quadratic. The quartic fit is nearing significance, but it can be 
up to you whether the cubic or the quartic will be better.You could also find the
best degree polynomial fit by using Cross-Validation, butthat is not covered here.

Step Functions are rather easy to set up; we basically only need `cut()` to help
us split up the predictor that we want to create our 'steps' from. It works by converting
a numeric into an ordered factor, which will then be 'dummified' when we fit out model

```{r}
# Example how cut() works
table(cut(age, 4))

# Step Function Estimation
lm(wage ~ cut(age, 4), data = Wage) %>%
  tidy()
```

## 7.8.2 Splines

In this section we will be using the `splines` package. If you need a refresher
like I did, refer back to page 273 of the text to see exactly what we will be 
fitting, or check out [this](http://people.stat.sfu.ca/~cschwarz/Consulting/Trinity/Phase2/TrinityWorkshop/Workshop-handouts/TW-04-Intro-splines.pdf) web page for more insight. We'll be using `bs()` to help us 
create the cubic spline basis on which the model will be estimated. The formulation
of these bases is far outside the scope of ISLR and this script, but you can check
out [Chapter 5 of Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)
for a more detailed description of the matter.


```{r}
# Fit our model (a cubic spline)
fit <- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = Wage)

# Observe coefficients
tidy(fit)
```

Just to be super clear, this is the model that we just fit (I know this doesn't
look all that pretty, but github-flavored markdown doesn't handle in-line equations easily): \n
$$age = \beta_0 + \beta_1(age) + \beta_2(age^2) + \beta_3(age^3) + \beta_4(age-\xi_1)^3 + \beta_5(age-\xi_2)^3 + \beta_6(age-\xi_3)^3 + \epsilon$$
Recall that the last three terms (excluding the error) represent the truncated power
basis for each knot. These ensure the continuity of the function at each of the knots
through the first and second derivative. 

We will now take a look at the fitted curve in relation to the `Wage` data
```{r, echo = F}
Wage %>%
  augment(fit, .) %>%
  mutate(fitted_low = .fitted - 2 * .se.fit,
         fitted_high = .fitted + 2 * .se.fit) %>%
  ggplot() +
  geom_point(aes(age, wage), cex = .5) + 
  geom_line(aes(age, .fitted), col = 'blue') +
  geom_line(aes(age, fitted_low), col = 'blue', linetype = 'dashed') +
  geom_line(aes(age, fitted_high), col = 'blue', linetype = 'dashed') +
  labs(title = "Cubic Spline Results") + 
  theme(plot.title = element_text(hjust = .5))
```

Notice in the plot how the confidence intervals start straying out from the actual
fit. This is a common problem with cubic splines, and to remedy that we can enforce
constraints on the form of the function outside of the boundary knots. These are called
__natural splines__. We use `ns()` to help us create the basis matrix for our natural 
cubic spline before plotting the results and comparing the performance of the two models

```{r}
natural_spline <- lm(wage ~ ns(age, df = 4), data = Wage)

# Observe coefficients
tidy(natural_spline)
```


```{r, echo = FALSE}
Wage %>%
  augment(natural_spline, .) %>%
  mutate(fitted_low = .fitted - 2 * .se.fit,
         fitted_high = .fitted + 2 * .se.fit) %>%
  ggplot() +
  geom_point(aes(age, wage), cex = .5) + 
  geom_line(aes(age, .fitted), col = 'red') +
  geom_line(aes(age, fitted_low), col = 'red', linetype = 'dashed') +
  geom_line(aes(age, fitted_high), col = 'red', linetype = 'dashed') +
  labs(title = "Natural Spline Results") + 
  theme(plot.title = element_text(hjust = .5))
```
 
As you can see, there is not a whole lot of visual difference other than the fact
that the confidence intervals near the boundary are a little more under control.
This is fine, as that is all we set out to accomplish.

We finish this section with a quick implementation of `smooth.spline()`, which as
the name suggests, will help us fit a smooth spline to wage. This function allows
for cross-validation for the best selection of the penalty parameter $\lambda$,
so we will do an example with and without it.

```{r, warning = F}
ss <- smooth.spline(age, wage, df = 16)
ss_cv <- smooth.spline(age, wage, cv = TRUE)
```

```{r, echo = F}
plot(Wage$age, Wage$wage, xlim = Wage$agelims, cex = .5, col = 'darkgrey')
title("Smoothing Splines")
lines(ss, col = 'red', lwd = 2)
lines(ss_cv, col = 'blue', lwd = 2)
legend('topright', legend = c("16 DF", "6.8 DF"),
       col = c("red", "blue"), lty = 1, lwd = 2, cex = .8)

```

We will be skipping local polynomial regression fitting in this write up of the lab
but if you are interested, go ahead and check out `loess()`. The syntax is pretty 
easy to digest and its implementation is very similar to `lm()` and `glm()`

***

## 7.8.3 General Additive Models (GAMs)

Our first example will build off of the previous section. That is, we will use 
natural splines of `age` and `year`, along with `education` to fit a model for `wage`.
We can accomplish this with the tools that we've learned so far.

```{r}
g1 <- lm(wage ~ ns(year, 4) + ns(age, 5) + education, data = Wage)

tidy(g1)
```

Next, we use smoothing splines instead of natural cubic ones. To do this,
we will use a new library: `gam`. Note that `s()` is used to specify a smoothing
spline instead of `smooth.spline()` in the previous section. `s()` is local to 
the `gam` library and is specifically used within the `gam()` function.

```{r}
g2 <- gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage)
```

Let's check out the results of the natural spline GAM (they are both pretty similar):

```{r, echo = FALSE}
par(mfrow=c(1,3))
plot(g2, se = TRUE, col = "blue")
```

`year` appears to be approximately linear in wage. Is it even necessary to create
a natural spline for it and can we just model a linear function of year? Let's test 
to see which variation will perform best:

```{r}
g1 <- gam(wage ~ s(age,5) + education, data = Wage)    # Exclude `year`
g2 <- gam(wage ~ year + s(age, 5) + education, data = Wage) # Wage linear in `year`
g3 <- gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage) # Natural spline for `year`

anova(g1, g2, g3)


```


According to these results, we do have evidence to suggest that excluding `year` 
would not be a good choice. In addition, we do not have enough evidence to suggest
that including the natural spline function for `year` would lead to a better performing
model.

We can also using local regression fits as building blocks in GAMs using `lo()` 
within the call to `gam()`

```{r}
gam_lo <- gam(wage ~ s(year, df = 4) + lo(age, span = .7) + education,
              data = Wage)

par(mfrow = c(1,3))
plot.gam(gam_lo, se = TRUE, col = 'green')

```

`lo()` can also be used to create interactions between variables. The `akima` 
package can be used to help us visualize the results of these interactions,
but we do not explore that here.

```{r}
gam_lo_i <- gam(wage ~ lo(year, age, span = .5) + education,
                data = Wage)
```

As a final example, we will estimate a logistic regression GAM using an indicator
for `Wage > 250` as our dependent variable.

```{r}
gam_lr <- gam(I(wage>250) ~ year + s(age, df = 5) + education, 
              family = binomial, data = Wage)

par(mfrow = c(1,3))
plot(gam_lr, se = TRUE, col = 'green')

```