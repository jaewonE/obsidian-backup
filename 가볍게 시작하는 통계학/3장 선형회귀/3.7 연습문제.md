1 답변
귀무가설: TV, radio, newspaper 광고는 판매량에 영향을 주지 않는다.
대립가설: TV, radio, newspaper 광고는 판매량에 영향을 준다.

newspaper의 p-값은 큼으로 newspaper 광고는 판매량에 영향을 주지 않는다. 
반대로 TV와 radio 광고는 판매량에 유의미한 영향을 준다. 

2 답변
KNN 분류기: [[KNN K-최근접이웃]]
KNN 회귀분석 : KNN을 이용한 회귀 분석(알아보기)

3 답변
문제에 근거하여 아래와 같이 식을 작성 할 수 있다.
$$y_i = 50 + 20X_1 + 0.07X_2 + 35X_3 + 0.01X_1X_2 -10X_1X_3 +\epsilon$$
$$y_i\begin{cases}\,\,85 + (10+0.01X_2)X_1 + 0.07X_2 +\epsilon & \text { i 번째 사람이 여성인 경우 } \\\\ \,\,50 + (20+ 0.01X_2)X_1 + 0.07X_2 +\epsilon & \text{ i 번째 사람이 남성인 경우}\end{cases}$$
a번
i. $X_1$ 과 $X_2$의 값이 고정임으로 상호작용항인 $x_4$ 또한 고정이다. 여성일 경우 1임으로 예 y의 값이 남성일 경우보다 크다. 따라서 남성이 여성보다 평균적으로 수입이 더 적다(?)
> #질문 
> 그렇다면 남성이 1 이고 여성이 0으로 설정하면 값이 뒤바뀌는 것인가? 아무래도 $X_5$에 따른 값이 변화를 물어보는 것 같은데 어떻게 $X_5$값을 도출 할 수 있는지 의문.
![[KakaoTalk_Photo_2023-02-28-14-40-07.jpeg]]
ii: i번의 반대 질문임으로 참.
iii: 위와 마찬가지의 논리로 여성일 경우 1임으로, GPA와 성별의 상호작용항인 $X_5$값이 여성일 경우 더 크다. 이때 $X_5$의 계수($\beta_5$)는 음수임으로 $X_5$의 값이 커질수록 예측된 y값이 작아짐으로 참이다.
iiii: iii와 반대 질문임으로 거짓.

b번
IQ가 110이고 GPA가 4.0임으로 식은 아래와 같이 다시 작성할 수 있다.
$$y_i = 137.7 + 35X_3 + 0.01X_4 -10X_5 +\epsilon$$
$$y_i\begin{cases}\,\,137.1 +\epsilon & \text { i 번째 사람이 여성인 경우 } \\\\ \,\,142.1 +\epsilon & \text{ i 번째 사람이 남성인 경우}\end{cases}$$


4번
(a). 다중선형회귀의 RSS가 더 작다. 더 많이 적합



7번
Chapter 3, Problem 7
========================================================

Generic Solution
--------------------------------------------------------

**Proposition**: Prove that in case of simple linear regression:

$$ y = \beta_0 + \beta_1 x + \varepsilon $$

the $R^2$ is equal to correlation between X and Y squared, e.g.:

$$ R^2 = corr^2(x, y) $$

We'll be using the following definitions to prove the above proposition.

**Def**:
$$ R^2 = \frac{TSS - RSS}{TSS} $$

**Def**:
$$ TSS = \sum (y_i - \bar{y})^2 \label{TSS} $$

**Def**:
$$ RSS = \sum (y_i - \hat{y}_i)^2 \label{RSS} $$ 

**Def**:
$$
\begin{align}
  corr(x, y) &= \frac{\sum (x_i - \bar{x}) (y_i - \bar{y})}
                     {\sigma_x \sigma_y} \\
  \sigma_x^2 &= \sum (x_i - \bar{x})^2 \\
  \sigma_y^2 &= \sum (y_i - \bar{y})^2
\end{align}
$$

**Proof**:

Substitute defintions of TSS and RSS into $R^2$:

$$
R^2 = \frac{\sum (y_i - \bar{y})^2 - \sum (y_i - \hat{y}_i)^2}
           {\sum (y_i - \bar{y})^2}
$$

Let's work on the numerator:

$$
\begin{align}
  A &= \sum (y_i - \bar{y})^2 - \sum (y_i - \hat{y}_i)^2 \\
    &= \sum \left[ (y_i - \bar{y}) - (y_i - \hat{y}_i) \right] 
            \left[ (y_i - \bar{y}) + (y_i - \hat{y}_i) \right] \\
    &= \sum (\hat{y}_i - \bar{y})
            (2y_i - \bar{y} - \hat{y}_i)
\end{align}
$$

Recall that:

$$
\begin{align}
  \hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \label{beta0} \\
  \hat{\beta}_1 &= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}
                        {\sum (x_j - \bar{x})^2}
\end{align}
$$

Substitute the expression for $\hat{\beta}_0$ into $\hat{y}_i$:

$$
\begin{align}
  \hat{y}_i &= \hat{\beta}_0 + \hat{\beta}_1 x_i \\
            &= \bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 x_i \\
            &= \bar{y} + \hat{\beta}_1 (x_i - \bar{x})
\end{align}
$$

Let's analyze two terms from $A$:

$$
\begin{align}
         \hat{y}_i - \bar{y} &= \hat{\beta}_1 (x_i - \bar{x}) \\
  2y_i - \bar{y} - \hat{y}_i &= 2y_i - \bar{y} - \bar{y} -
                                \hat{\beta}_1 (x_i - \bar{x}) \\
                             &= 2(y_i - \bar{y}) - 
                                \hat{\beta}_1 (x_i - \bar{x}) 
\end{align}
$$

and substitute these expressions back into $A$:

$$
\begin{align}
  A &= \sum \hat{\beta}_1 (x_i - \hat{x})
            \left[ 2(y_i - \bar{y}) - \hat{\beta}_1 (x_i - \bar{x}) \right] \\
    &= \hat{\beta}_1 \sum (x_i - \bar{x})
                          \left[ 2(y_i - \bar{y}) -
                                 \hat{\beta}_1 (x_i - \bar{x}) \right] \\
    &= \hat{\beta}_1
       \left[ 2 \sum (x_i - \bar{x})(y_i - \bar{y}) -
              \hat{\beta}_1 \sum (x_i - \bar{x})^2 \label{A4} \right]
\end{align}
$$

Using formula for $\hat{\beta}_1$ it is easy to see that the last term is
nothing but:

$$ \sum (x_i - \bar{x}) (y_i - \bar{y}) $$

Thus, we get:

$$
\begin{align}
  A &= \hat{\beta}_1 \sum (x_i - \bar{x}) (y_i - \bar{y}) \\
    &= \frac{\left[ \sum (x_i - \bar{x}) (y_i - \bar{y}) \right]^2}
            {\sum (x_j - \bar{x})^2}
\end{align}
$$

Plug the final expression for $A$ back into $R^2$:

$$
R^2 = \frac{\left[ \sum (x_i - \bar{x}) (y_i - \bar{y}) \right]^2}
           {\sum (x_j - \bar{x})^2 \sum (y_k - \bar{y})^2}
$$

Compare this to the definition of correlation and get:

$$ R^2 = corr^2(x, y) $$

8번 (Auto data set을 사용하라)
a.
[R]
```
#ISLR 패키지를 불러온다
library(ISLR)

#Auto 데이터셋을 불러온다
data(Auto)

#lm()함수를 사용해서 mpg(연비)와 horsepower(마력) 사이의 선형회귀 모델을 만들고, data 매개변수를 사용하여 Auto 데이터셋을 지정하여, 모델을 mpg_power에 저장한다.
mpg_power <- lm(mpg~horsepower, data=Auto)

#summary()함수를 사용하여 mpg_power 모델에 대한 요약 통계를 출력한다.
summary(mpg_power)
```
출력
![[Pasted image 20230302130629.png]]
1. Call: 모델의 호출 정보
	* lm() 함수의 formula 매개변수와 data 매개변수를 사용하여 모델이 생성되었다.

2. Residuals: 모델에서 예측한 값과 실제 값 사이의 [[잔차(residual)]] -> 모델의 적합도 평가 가능
	* 각 항목의 의미
		- Min: 잔차의 최솟값
		- 1Q: 제 1 사분위수 (잔차의 하위 25% 값)
		- Median: 잔차의 중앙값
		- 3Q: 제 3 사분위수 (잔차의 상위 75% 값)
		- Max: 잔차의 최댓값
	
	* 모델의 적합도 평가
		* Q-Q plot (시각적인 평가)
			* 잔차의 분포를 표준정규분포와 비교하여 그린 그래프이다.
				- 모델이 데이터를 잘 적합했다면, 잔차는 평균값이 0이 되는 정규분포에 따를 것이다. 즉, 잔차들이 랜덤하게 분포되어 있어야 한다. 따라서 잔차의 분포가 이상적일 경우, Residuals의 Q-Q plot을 보면 y=x의 직선에 가까운 분포가 되어야 한다.  
				
		* 표준편차가 작을수록 모델이 데이터를 잘 적합시킨다.
		* [[오차제곱합(SSE)]]이 작을수록 모델이 데이터를 잘 적합시킨다. 
#재정리 
3. Coefficients: [[회귀계수]]에 대한 정보를 제공 -> 각 독립변수가 종속변수에 어떤 영향을 미치는지 
	* Estimate: 회귀계수의 추정값
	* Std. Error: 추정값의 표준오차
	* t value: 추정값이 0일 때, 표준오차를 나눈 t-통계량의 값
	* Pr(>|t|): t-통계량이 0일 때의 p-value

4. Residual standard error[[잔차표준오차(RSE)]]: 잔차의 표준편차 -> 모델의 적합도 평가 가능

5. Multiple R-squared: [[결정계수(R2)]]값, 1에 가까울수록 모델이 데이터를 잘 설명

6. Adjusted R-squared: [[조정된 결정계수(Adjusted R-Squared)]]값.

7. F-statistic: 회귀 모델의 선형적합도를 평가하는 F-통계량의 값
	* F-statistic: F-통계량 결과, 높을수록 모델이 데이터를 잘 설명함
	* α and β DF: 이 F-통계량을 계산하기 위해 사용된 [[자유도(degree of freedom)]]
		* α: 모델에서 추정한 매개변수의 수
		* β: 잔차의 자유도
	* p-value: F-통계량이 0일 때의 p-value. 작을수록 모델이 데이터에 잘 적합

[R]
```
#Q-Q plot를 그리기 위해 residuals() 함수를 사용해 모델에서 잔차를 추출한다.
residuals <- residuals(mpg_power)

# Q-Q plot 그리기
qqnorm(residuals)
qqline(residuals)

#모델의 SEE를 출력한다.
summary(mpg_power)$sigma
```
2. 의 Q-Q plot의 결과는 y=x의 직선 모양을 따른다. 
	![[Pasted image 20230302142805.png]]
	- mpg_power 모델은 데이터를 잘 설명한다고 생각할 수 있다.
3. **의 Pr(>|t|)값 중 horsepower의 계수에 해당하는 값이 0에 가깝다.**
	* **회귀모델에서 mpg와 horsepower간에 강한 관련이 있을 가능성이 높다, 즉 관계가 유의하다고 생각할 수 있다. **

4. 의 Residual standard error항목의 결과는 4.906 on 390 degrees of freedom이다.
	- [[잔차표준오차(RSE)]]는 4.906이며, 이 잔차의 표준오차를 구하기 위해 사용된 [[자유도(degree of freedom)]]는 390이다. 자유도는 모델에서 추정된 매개변수의 수에 따라 결정된다.
	- [[잔차표준오차(RSE)]]가 작을수록 모델이 데이터에 잘 적합되었다고 볼 수 있다.
	- [[자유도(degree of freedom)]]가 클수록 모델이 더 정확하게 추정될 가능성이 높다. 하지만 자유도가 너무 크면 과적합(overfitting)의 위험이 있다. 
	* 데이터를 상대적으로 잘 적합키고 안정적으로 예측을 할 수 있는 모델일 가능성이 높다.

**5. 의 Multiple R-squared항목의 결과는 0.6059이다. 
	* 이 값이 60%의 분산을 horsepower로 설명할 수 있다.**

6. 의 Adjusted R-squared항목의 결과는 0.6049이다.
	* 1에 가까울수록 모델이 데이터를 잘 설명한다.
	
7. 의 F-statistic항목의 결과는 599.7 on 1 and 390 DF, p-value: < 2.2e-16이다.
	* F-statistic은 599.7이며, 이 값이 높을수록 모델이 데이터를 잘 설명한다.
	* 1 and 390 DF: 이 F-통계량을 계산하기 위해 사용된 [[자유도(degree of freedom)]]
		* 1: 이 모델에서는 하나의 독립변수와 하나의 종속변수를 사용했기 때문
		* 390: 잔차의 자유도가 390이다. 
	* p-value는 < 2.2e-16 이며, 이 값이 작을수록 모델이 데이터를 잘 설명한다.
		* 값이 매우 작기 때문에 모델이 종속 변수를 설명하는데 있어서 유의미한 영향을 미친다.
		* 예측 성능이 유의미하며, 모델이 데이터를 잘 설명하고 있다. 

(i) 
**Pr(>|t|)값 중 horsepower의 계수에 해당하는 값이 0에 가깝다.**
회귀모델에서 mpg와 horsepower간에 강한 관련이 있을 가능성이 높다, 즉 관계가 유의하다고 생각할 수 있다. 

(ii)
Multiple R-squared항목의 결과는 0.6059이다. 
* 모델에서 mpg의 분산의 60%가 horsepower로 설명할 될 수 있다
* 평균과 [[잔차표준오차(RSE)]]를 사용해서 평균mpg값에 대한 백분률 오차를 구할 수 있다.
[R]
```
#mpg의 평균을 계산한다. na.rm=T옵션은 결측값을 제외하는 옵션이다.
mean(Auto$mpg, na.rm=T)
```
* Residual standard error[[잔차표준오차(RSE)]]항목의 결과는 4.906이다.
* 평균 mpg값에 
[R]
```
4.906/mean(Auto$mpg, na.rm=T) * 100.0
```
* 출력
![[Pasted image 20230302193521.png]]
* 즉, 모델이 예측한 값이 평균 mpg 값에서 약 20.88%의 오차를 가지고 있다는 것을 알 수 있다. 