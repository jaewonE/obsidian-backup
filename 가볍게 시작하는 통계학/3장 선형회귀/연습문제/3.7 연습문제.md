
7번
Chapter 3, Problem 7
========================================================

Generic Solution
--------------------------------------------------------

**Proposition**: Prove that in case of simple linear regression:

$$ y = \beta_0 + \beta_1 x + \varepsilon $$

the $R^2$ is equal to correlation between X and Y squared, e.g.:

$$ R^2 = corr^2(x, y) $$

We'll be using the following definitions to prove the above proposition.

**Def**:
$$ R^2 = \frac{TSS - RSS}{TSS} $$

**Def**:
$$ TSS = \sum(y_i - \bar{y})^2 \label{TSS} $$

**Def**:
$$ RSS = \sum (y_i - \hat{y}_i)^2 \label{RSS} $$ 

**Def**:
$$
\begin{align}
  corr(x, y) &= \frac{\sum (x_i - \bar{x}) (y_i - \bar{y})}
                     {\sigma_x \sigma_y} \\
  \sigma_x^2 &= \sum (x_i - \bar{x})^2 \\
  \sigma_y^2 &= \sum (y_i - \bar{y})^2
\end{align}
$$

**Proof**:

Substitute defintions of TSS and RSS into $R^2$:

$$
R^2 = \frac{\sum (y_i - \bar{y})^2 - \sum (y_i - \hat{y}_i)^2}
           {\sum (y_i - \bar{y})^2}
$$

Let's work on the numerator:

$$
\begin{align}
  A &= \sum (y_i - \bar{y})^2 - \sum (y_i - \hat{y}_i)^2 \\
    &= \sum \left[ (y_i - \bar{y}) - (y_i - \hat{y}_i) \right] 
            \left[ (y_i - \bar{y}) + (y_i - \hat{y}_i) \right] \\
    &= \sum (\hat{y}_i - \bar{y})
            (2y_i - \bar{y} - \hat{y}_i)
\end{align}
$$

Recall that:

$$
\begin{align}
  \hat{\beta}_0 &= \bar{y} - \hat{\beta}_1 \bar{x} \label{beta0} \\
  \hat{\beta}_1 &= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}
                        {\sum (x_j - \bar{x})^2}
\end{align}
$$

Substitute the expression for $\hat{\beta}_0$ into $\hat{y}_i$:

$$
\begin{align}
  \hat{y}_i &= \hat{\beta}_0 + \hat{\beta}_1 x_i \\
            &= \bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 x_i \\
            &= \bar{y} + \hat{\beta}_1 (x_i - \bar{x})
\end{align}
$$

Let's analyze two terms from $A$:

$$
\begin{align}
         \hat{y}_i - \bar{y} &= \hat{\beta}_1 (x_i - \bar{x}) \\
  2y_i - \bar{y} - \hat{y}_i &= 2y_i - \bar{y} - \bar{y} -
                                \hat{\beta}_1 (x_i - \bar{x}) \\
                             &= 2(y_i - \bar{y}) - 
                                \hat{\beta}_1 (x_i - \bar{x}) 
\end{align}
$$

and substitute these expressions back into $A$:

$$
\begin{align}
  A &= \sum \hat{\beta}_1 (x_i - \hat{x})
            \left[ 2(y_i - \bar{y}) - \hat{\beta}_1 (x_i - \bar{x}) \right] \\
    &= \hat{\beta}_1 \sum (x_i - \bar{x})
                          \left[ 2(y_i - \bar{y}) -
                                 \hat{\beta}_1 (x_i - \bar{x}) \right] \\
    &= \hat{\beta}_1
       \left[ 2 \sum (x_i - \bar{x})(y_i - \bar{y}) -
              \hat{\beta}_1 \sum (x_i - \bar{x})^2 \label{A4} \right]
\end{align}
$$

Using formula for $\hat{\beta}_1$ it is easy to see that the last term is
nothing but:

$$ \sum (x_i - \bar{x}) (y_i - \bar{y}) $$

Thus, we get:

$$
\begin{align}
  A &= \hat{\beta}_1 \sum (x_i - \bar{x}) (y_i - \bar{y}) \\
    &= \frac{\left[ \sum (x_i - \bar{x}) (y_i - \bar{y}) \right]^2}
            {\sum (x_j - \bar{x})^2}
\end{align}
$$

Plug the final expression for $A$ back into $R^2$:

$$
R^2 = \frac{\left[ \sum (x_i - \bar{x}) (y_i - \bar{y}) \right]^2}
           {\sum (x_j - \bar{x})^2 \sum (y_k - \bar{y})^2}
$$

Compare this to the definition of correlation and get:

$$ R^2 = corr^2(x, y) $$

8번 (Auto data set을 사용하라)
a.
[R]
```
#ISLR 패키지를 불러온다
library(ISLR)

#Auto 데이터셋을 불러온다
data(Auto)

#lm()함수를 사용해서 mpg(연비)와 horsepower(마력) 사이의 선형회귀 모델을 만들고, data 매개변수를 사용하여 Auto 데이터셋을 지정하여, 모델을 mpg_power에 저장한다.
mpg_power <- lm(mpg~horsepower, data=Auto)

#summary()함수를 사용하여 mpg_power 모델에 대한 요약 통계를 출력한다.
summary(mpg_power)
```
출력
![[Pasted image 20230302130629.png]]
1. Call: 모델의 호출 정보
	* lm() 함수의 formula 매개변수와 data 매개변수를 사용하여 모델이 생성되었다.

2. Residuals: 모델에서 예측한 값과 실제 값 사이의 [[잔차(residual)]] -> 모델의 적합도 평가 가능
	* 각 항목의 의미
		- Min: 잔차의 최솟값
		- 1Q: 제 1 사분위수 (잔차의 하위 25% 값)
		- Median: 잔차의 중앙값
		- 3Q: 제 3 사분위수 (잔차의 상위 75% 값)
		- Max: 잔차의 최댓값
	
	* 모델의 적합도 평가
		* Q-Q plot (시각적인 평가)
			* 잔차의 분포를 표준정규분포와 비교하여 그린 그래프이다.
				- 모델이 데이터를 잘 적합했다면, 잔차는 평균값이 0이 되는 정규분포에 따를 것이다. 즉, 잔차들이 랜덤하게 분포되어 있어야 한다. 따라서 잔차의 분포가 이상적일 경우, Residuals의 Q-Q plot을 보면 y=x의 직선에 가까운 분포가 되어야 한다.  
				
		* 표준편차가 작을수록 모델이 데이터를 잘 적합시킨다.
		* [[오차제곱합(SSE)]]이 작을수록 모델이 데이터를 잘 적합시킨다. 
#재정리 
3. Coefficients: [[회귀계수]]에 대한 정보를 제공 -> 각 독립변수가 종속변수에 어떤 영향을 미치는지 
	* Estimate: 회귀계수의 추정값
	* Std. Error: 추정값의 표준오차
	* t value: 추정값이 0일 때, 표준오차를 나눈 t-통계량의 값
	* Pr(>|t|): t-통계량이 0일 때의 p-value

4. Residual standard error[[잔차표준오차(RSE)]]: 잔차의 표준편차 -> 모델의 적합도 평가 가능

5. Multiple R-squared: [[결정계수(R2)]]값, 1에 가까울수록 모델이 데이터를 잘 설명

6. Adjusted R-squared: [[조정된 결정계수(Adjusted R-Squared)]]값.

7. F-statistic: 회귀 모델의 선형적합도를 평가하는 F-통계량의 값
	* F-statistic: F-통계량 결과, 높을수록 모델이 데이터를 잘 설명함
	* α and β DF: 이 F-통계량을 계산하기 위해 사용된 [[자유도(degree of freedom)]]
		* α: 모델에서 추정한 매개변수의 수
		* β: 잔차의 자유도
	* p-value: F-통계량이 0일 때의 p-value. 작을수록 모델이 데이터에 잘 적합

[R]
```
#Q-Q plot를 그리기 위해 residuals() 함수를 사용해 모델에서 잔차를 추출한다.
residuals <- residuals(mpg_power)

# Q-Q plot 그리기
qqnorm(residuals)
qqline(residuals)

#모델의 SEE를 출력한다.
summary(mpg_power)$sigma
```
2. 의 Q-Q plot의 결과는 y=x의 직선 모양을 따른다. 
	![[Pasted image 20230302142805.png]]
	- mpg_power 모델은 데이터를 잘 설명한다고 생각할 수 있다.
3. **의 Pr(>|t|)값 중 horsepower의 계수에 해당하는 값이 0에 가깝다.**
	* **회귀모델에서 mpg와 horsepower간에 강한 관련이 있을 가능성이 높다, 즉 관계가 유의하다고 생각할 수 있다. **

4. 의 Residual standard error항목의 결과는 4.906 on 390 degrees of freedom이다.
	- [[잔차표준오차(RSE)]]는 4.906이며, 이 잔차의 표준오차를 구하기 위해 사용된 [[자유도(degree of freedom)]]는 390이다. 자유도는 모델에서 추정된 매개변수의 수에 따라 결정된다.
	- [[잔차표준오차(RSE)]]가 작을수록 모델이 데이터에 잘 적합되었다고 볼 수 있다.
	- [[자유도(degree of freedom)]]가 클수록 모델이 더 정확하게 추정될 가능성이 높다. 하지만 자유도가 너무 크면 과적합(overfitting)의 위험이 있다. 
	* 데이터를 상대적으로 잘 적합키고 안정적으로 예측을 할 수 있는 모델일 가능성이 높다.

**5. 의 Multiple R-squared항목의 결과는 0.6059이다. 
	* 이 값이 60%의 분산을 horsepower로 설명할 수 있다.**

6. 의 Adjusted R-squared항목의 결과는 0.6049이다.
	* 1에 가까울수록 모델이 데이터를 잘 설명한다.
	
7. 의 F-statistic항목의 결과는 599.7 on 1 and 390 DF, p-value: < 2.2e-16이다.
	* F-statistic은 599.7이며, 이 값이 높을수록 모델이 데이터를 잘 설명한다.
	* 1 and 390 DF: 이 F-통계량을 계산하기 위해 사용된 [[자유도(degree of freedom)]]
		* 1: 이 모델에서는 하나의 독립변수와 하나의 종속변수를 사용했기 때문
		* 390: 잔차의 자유도가 390이다. 
	* p-value는 < 2.2e-16 이며, 이 값이 작을수록 모델이 데이터를 잘 설명한다.
		* 값이 매우 작기 때문에 모델이 종속 변수를 설명하는데 있어서 유의미한 영향을 미친다.
		* 예측 성능이 유의미하며, 모델이 데이터를 잘 설명하고 있다. 

(i) 
**Pr(>|t|)값 중 horsepower의 계수에 해당하는 값이 0에 가깝다.**
회귀모델에서 mpg와 horsepower간에 강한 관련이 있을 가능성이 높다, 즉 관계가 유의하다고 생각할 수 있다. 

(ii)
Multiple R-squared항목의 결과는 0.6059이다. 
* 모델에서 mpg의 분산의 60%가 horsepower로 설명할 될 수 있다
* 평균과 [[잔차표준오차(RSE)]]를 사용해서 평균mpg값에 대한 백분률 오차를 구할 수 있다.
[R]
```
#mpg의 평균을 계산한다. na.rm=T옵션은 결측값을 제외하는 옵션이다.
mean(Auto$mpg, na.rm=T)
```
* Residual standard error[[잔차표준오차(RSE)]]항목의 결과는 4.906이다.
* 평균 mpg값에 대한 백분률 오차(RSE/mean)*100%
[R]
```
4.906/mean(Auto$mpg, na.rm=T) * 100.0
```
* 출력
![[Pasted image 20230302193521.png]]
* 즉, 모델이 예측한 값이 평균 mpg 값에서 약 20.88%의 오차를 가지고 있다는 것을 알 수 있다. 

(iii)
모델의 식은 다음과 같다. 
y = -0.157845x + 39.935861
즉, 음의 선형 관계를 가진다. 

(iv)
모델의 식은 다음과 같다. 
y = -0.157845x + 39.935861
여기에서 y는 mpg이고, x는 horsepower이다. 

horsepower(x)가 98%인 경우 예측되는 mpg(y)는 24.467077, 약 24.47이다. 

문제에서는 신뢰 구간과 예측 구간을 물어봤기 때문에, predict()함수를 사용한다. 
[R]
```
#horsepower가 98일 경우 모델의 mpg값의 예측구간과 95%신뢰구간
predict(mpg_power, data.frame(horsepower=c(98)), interval='prediction')
predict(mpg_power, data.frame(horsepower=c(98)), interval='confidence')
```

출력
![[Pasted image 20230302194356.png]]

* horsepower가 98일 경우, 예측된 mpg는 약 24.27이다. 
* 이 예측에 대한 95% 신뢰 구간은 (23.97308, 24.96108)이다. 
* 95% 예측 구간은 (14.8094, 34.12476)으로 더 넓으며, 개별 관측치가 구간 밖에 있을 가능성을 포함한다.

b
[R]
```
#Auto 데이터셋으로부터 최소제곱회귀선을 구해 scatter plot을 그리고 그래프 이름을 설정
plot(mpg~horsepower,main= "Scatter plot of mpg & horsepower", data=Auto)

#회귀선을 추가
abline(lm(mpg~horsepower, data=Auto), lwd =3, col ="red")
```
출력
![[Pasted image 20230302200133.png]]
